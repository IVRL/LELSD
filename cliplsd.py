""" CLIP LSD PyTorch model.
    By Albert Aillet, Shady Moukel, Paulina Tao
    Heavily used code from "Optimizing Latent Space Directions For GAN-based Local Image Editing"
    By Ehsan Pajouheshgar, Tong Zhang, and Sabine Susstrunk
    https://arxiv.org/abs/2111.12583
"""
from lelsd import LELSD

import json
import os
from datetime import datetime

import numpy as np
import torch
import torchvision
from torch.autograd import Variable
from tqdm import tqdm

from utils.segmentation_utils import GANLinearSegmentation

try:
    from tensorboardX import SummaryWriter
except:
    from torch.utils.tensorboard import SummaryWriter


class CLIPLSD(LELSD):
    """
    This class tries to find directions in the latent space of a pretrained GAN
    that if added to the initial latent vector, the change in the output image
    will be localized to a specific semantic part of the object.
    For example you can use this model to find latent directions that only change
    the mouth, or the nose of the generated face.
    For fitting this model, a pretrained STYLEGAN2, and CLIP model are required.
    Parameters
    ----------
    device: pytorch device
        Device used for performing the computation.
    localization_layers: list, required
        The layers that will be used in the Localization score.
        The Localozation Score tries to maximize the ratio of the
        changes in the desired part to changes outside
        the desired part. The mask determining the desired part will
        be bilinearly [up-down]-sampled to have the same shape as layer feature maps.
        Note that the layer indices should be ascendingly sorted.
    semantic_parts: list, required
        List of strings. This list will determine the semantic parts of the output image we want to modify and edit. 
        For fitting the model a clip model is required.
    """

    def __init__(self, device, localization_layers, semantic_parts, **kwargs):
        LELSD.__init__(self, device, localization_layers, semantic_parts, **kwargs)
        delattr(self, 'mode')
        delattr(self, 'mask_aggregation')
        delattr(self, 'semantic_parts')
        delattr(self, 'combine_mask')
        # add needed attributes

    def fit(self, gan_sample_generator, clip_model, num_batches, num_lr_halvings=4, batch_size=None,
            pgbar=False, summary=True, snapshot_interval=200):
        """
        This method will optimize the latent directions using samples generated by gan_sample_generator
        that will be rated by the clip model.
        Parameters
        ----------
        gan_sample_generator: SampleGenerator,
            This is a wrapper class on top of the pretrained generator network.
        clip_model:
            # TODO: add description
        num_batches: int
            Number of batches used for optimizing the latent directions
        num_lr_halvings: int, default=4
            Number of times that the learning rate will be halved.
        batch_size: int, default=None
            The batch size used for the optimization.
            if not specified the class attribute will be used.
        pgbar: bool, default=False
            Whether to show the progress bar or not
        summary: bool, default=True
            Whether to write summaries or not
        snapshot_interval: int, default=200
            The interval between saving image snapshots.
        """
        if batch_size is None:
            batch_size = self.batch_size
        if summary:
            with open(os.path.join(self.log_dir, "config.json"), "w") as f:
                config_dict = self.__dict__.copy()
                del config_dict['device']
                del config_dict['latent_dirs']
                del config_dict['coefficient_sampling_dist']
                json.dump(config_dict, f, indent=4, sort_keys=True, default=lambda x: x.__name__)

            try:
                summary_writer = SummaryWriter(logdir=self.log_dir)
            except:
                summary_writer = SummaryWriter(self.log_dir)

        lr = self.lr
        lr_decay_steps = num_batches // (num_lr_halvings + 1)
        min_alpha_value = self.min_alpha_value
        max_alpha_value = self.max_alpha_value
        global_step = 0
        pbar = tqdm(range(54321, 54321 + num_batches, 1), disable=not pgbar, total=num_batches)
        pil_to_tensor = torchvision.transforms.ToTensor()

        if self.latent_space.startswith("S"):
            optimizer = torch.optim.Adam(self.latent_dirs, lr=lr)
        else:
            optimizer = torch.optim.Adam([self.latent_dirs], lr=lr)
        for i, seed in enumerate(pbar):
            if i % lr_decay_steps == (lr_decay_steps - 1):
                lr /= 2
                optimizer.param_groups[0]['lr'] = lr

            batch_data = gan_sample_generator.generate_batch(batch_size=batch_size, seed=seed, requires_grad=True,
                                                             return_all_layers=True, return_image=True)

            # TODO: here we have to use the CLIP model to get the semantics once

            gan_sample_generator.zero_grad()
            clip_model.zero_grad()
            alpha = self.sample_alpha(min_alpha_value, max_alpha_value, min_abs_value=self.min_abs_alpha_value)
            latent_coefs = self.coefficient_sampling_dist.sample().unsqueeze(1).to(self.device)  # [num_ws, 1]
            if not self.latent_space.startswith("S"):
                if self.latent_space.startswith("W"):
                    old_latent = batch_data['ws'].detach()
                    if self.latent_space == "W+":
                        latent_coefs = latent_coefs.unsqueeze(2)
                    latent_dir = torch.sum(self.latent_dirs * latent_coefs, dim=0, keepdim=True)
                    new_latent = self._move_latent_codes(old_latent, latent_dir, alpha)
                    new_batch_data = gan_sample_generator.generate_batch_from_ws(new_latent, requires_grad=True,
                                                                                 return_all_layers=True,
                                                                                 return_image=True)
                else:
                    old_latent = batch_data['z'].detach()
                    latent_dir = torch.sum(self.latent_dirs * latent_coefs, dim=0, keepdim=True)
                    new_latent = self._move_latent_codes(old_latent, latent_dir, alpha)
                    new_batch_data = gan_sample_generator.generate_batch_from_z(new_latent, requires_grad=True,
                                                                                return_all_layers=True,
                                                                                return_image=True)

            else:
                old_latent = [tmp.detach() for tmp in batch_data['s']]
                latent_dir = [torch.sum(u * latent_coefs, dim=0, keepdim=True) for u in self.latent_dirs]
                new_latent = self._move_latent_codes(old_latent, latent_dir, alpha,
                                                     layers_to_apply=self.s_layers_to_apply)
                ys_tuple = gan_sample_generator.s_to_ys(new_latent)
                new_batch_data = gan_sample_generator.generate_batch_from_ys(ys_tuple, requires_grad=True,
                                                                             return_all_layers=True,
                                                                             return_image=True)

            # TODO: here we have to use the CLIP model to get the semantics again

            # To maximize the Localization Score in localization layers
            # TODO: Ask the TA about this. What does layer_res do?
            # This part of the code goes through the layers and calculates the loss for each one.

            
            # TODO: here we should calculate the Loss from both sematic scores and id loss etc
            
            clip_loss = 0
            if self.latent_space == "W+":
                correlation_loss = 0
                for layer in range(self.n_layers):
                    correlation_loss += self.correlation_loss(self.latent_dirs[:, layer, :])
                correlation_loss /= self.n_layers
            elif self.latent_space.startswith("S"):
                correlation_loss = 0
                if self.s_layers_to_apply is None:
                    for latent_dir in self.latent_dirs:
                        correlation_loss += self.correlation_loss(latent_dir)
                    correlation_loss /= len(self.latent_dirs)
                else:
                    for layer in self.s_layers_to_apply:
                        correlation_loss += self.correlation_loss(self.latent_dirs[layer])
                    correlation_loss /= len(self.s_layers_to_apply)
            else:
                correlation_loss = self.correlation_loss(self.latent_dirs)
            loss = clip_loss + self.gamma_correlation * correlation_loss
            loss.backward()
            if self.unit_norm:
                optimizer.step()
                self.latent_dirs.data = self.latent_dirs.data / torch.linalg.norm(self.latent_dirs.data, dim=1,
                                                                                  keepdim=True)
                optimizer.zero_grad()
            else:
                pass
                optimizer.step()
                optimizer.zero_grad()

            if summary:
                if global_step % snapshot_interval == 0:
                    old_images = [pil_to_tensor(img.resize((256, 256))) for img in batch_data['image'][:4]]
                    new_images = [pil_to_tensor(img.resize((256, 256))) for img in new_batch_data['image'][:4]]
                    images = old_images + new_images
                    image_grid = torchvision.utils.make_grid(images, nrow=2)
                    torchvision.utils.save_image(image_grid,
                                                 os.path.join(self.log_dir, f"batch={i}-image_alpha={alpha:.2f}.jpg"))

                summary_writer.add_scalar("clip loss score", -clip_loss, global_step)
                summary_writer.add_scalar("correlation_loss", correlation_loss, global_step)
                summary_writer.add_scalar("loss", loss, global_step)
                summary_writer.add_scalar("alpha", alpha, global_step)
                summary_writer.add_scalar("lr", lr, global_step)
                global_step += 1

        if summary:
            summary_writer.close()
        if self.unit_norm:
            if not self.latent_space.startswith("S"):
                self.latent_dirs.data = self.latent_dirs.data / torch.linalg.norm(self.latent_dirs.data, dim=1,
                                                                                  keepdim=True)
            else:
                for l in range(len(self.latent_dirs)):
                    self.latent_dirs[l].data = self.latent_dirs[l].data / torch.linalg.norm(self.latent_dirs[l].data,
                                                                                            dim=1, keepdim=True)
    
