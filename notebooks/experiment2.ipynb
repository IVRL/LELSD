{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertaillet/CLIPLSD/blob/colab-setup/notebooks/experiment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Clone repo and from correct branch\n",
        "*   Download the pretrained stylegan2 model from https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4kq_s1FO81-G"
      },
      "id": "4kq_s1FO81-G"
    },
    {
      "cell_type": "code",
      "source": [
        "%rm CLIPLSD -r"
      ],
      "metadata": {
        "id": "gOjcRJwg2RYZ"
      },
      "id": "gOjcRJwg2RYZ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch colab-setup https://github.com/albertaillet/CLIPLSD/\n",
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl -P CLIPLSD/pretrained/stylegan2"
      ],
      "metadata": {
        "id": "xy-v9jJHBfFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54aee596-5bf5-4623-f0f6-d9e08780d3ba"
      },
      "id": "xy-v9jJHBfFT",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIPLSD'...\n",
            "remote: Enumerating objects: 244, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 244 (delta 56), reused 17 (delta 8), pack-reused 139\u001b[K\n",
            "Receiving objects: 100% (244/244), 36.66 MiB | 24.17 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "--2022-04-26 17:22:09--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 52.222.158.82, 52.222.158.47, 52.222.158.2, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|52.222.158.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381624121 (364M) [binary/octet-stream]\n",
            "Saving to: ‘CLIPLSD/pretrained/stylegan2/ffhq.pkl’\n",
            "\n",
            "ffhq.pkl            100%[===================>] 363.94M   136MB/s    in 2.7s    \n",
            "\n",
            "2022-04-26 17:22:12 (136 MB/s) - ‘CLIPLSD/pretrained/stylegan2/ffhq.pkl’ saved [381624121/381624121]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r CLIPLSD/requirements.txt\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "jLkrICI2BtY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f0fcc6-483e-4fc4-b137-5658a432159b"
      },
      "id": "jLkrICI2BtY8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 2)) (8.1.2)\n",
            "Requirement already satisfied: pillow==8.3.1 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 3)) (8.3.1)\n",
            "Requirement already satisfied: scipy==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: torch==1.9.1 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 5)) (1.9.1)\n",
            "Requirement already satisfied: requests==2.26.0 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 6)) (2.26.0)\n",
            "Requirement already satisfied: tqdm==4.62.2 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 7)) (4.62.2)\n",
            "Requirement already satisfied: ninja==1.10.2 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 8)) (1.10.2)\n",
            "Requirement already satisfied: matplotlib==3.4.2 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 10)) (2.9.0)\n",
            "Requirement already satisfied: imgui==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: glfw==2.2.0 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 12)) (2.2.0)\n",
            "Requirement already satisfied: pyopengl==3.1.5 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 13)) (3.1.5)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.4.3 in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 14)) (0.4.3)\n",
            "Requirement already satisfied: pyspng in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 15)) (0.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 16)) (3.13)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 18)) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 19)) (2019.12.20)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 20)) (3.2.5)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.7/dist-packages (from -r CLIPLSD/requirements.txt (line 21)) (0.1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->-r CLIPLSD/requirements.txt (line 5)) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->-r CLIPLSD/requirements.txt (line 6)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->-r CLIPLSD/requirements.txt (line 6)) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->-r CLIPLSD/requirements.txt (line 6)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.26.0->-r CLIPLSD/requirements.txt (line 6)) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r CLIPLSD/requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r CLIPLSD/requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r CLIPLSD/requirements.txt (line 9)) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2->-r CLIPLSD/requirements.txt (line 9)) (1.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0->-r CLIPLSD/requirements.txt (line 2)) (4.11.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.2->-r CLIPLSD/requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->-r CLIPLSD/requirements.txt (line 18)) (0.2.5)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips->-r CLIPLSD/requirements.txt (line 21)) (0.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0->-r CLIPLSD/requirements.txt (line 2)) (3.8.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CLIPLSD/"
      ],
      "metadata": {
        "id": "X65wnOhK_B-P",
        "outputId": "fbc544d6-5ca6-441a-b3e7-016dce16a7ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X65wnOhK_B-P",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CLIPLSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "ybJekDfDwfZ5"
      },
      "id": "ybJekDfDwfZ5",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8f050d55-2b6d-40cc-b81b-3c550279086d",
      "metadata": {
        "id": "8f050d55-2b6d-40cc-b81b-3c550279086d"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from IPython.display import Image \n",
        "\n",
        "import clip\n",
        "import models\n",
        "from utils.stylegan2_utils import StyleGAN2SampleGenerator\n",
        "from cliplsd import CLIPLSD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\", \".join(clip.available_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Bv3RepR0jw70",
        "outputId": "3add5193-3185-44ea-ab85-6f830d927018"
      },
      "id": "Bv3RepR0jw70",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RN50, RN101, RN50x4, RN50x16, RN50x64, ViT-B/32, ViT-B/16, ViT-L/14, ViT-L/14@336px'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "exp_dir = \"out\"\n",
        "G2 = models.get_model(\"stylegan2\", \"pretrained/stylegan2/ffhq.pkl\")\n",
        "sample_generator = StyleGAN2SampleGenerator(G=G2, device=device)\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")"
      ],
      "metadata": {
        "id": "iNqfpd6T8-4-"
      },
      "id": "iNqfpd6T8-4-",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_space = \"Z\"\n",
        "loss_function = \"L2\"\n",
        "num_latent_dirs = 1\n",
        "part_name = \"mouth\"\n",
        "lr = 0.001\n",
        "min_alpha_value = -1.0\n",
        "max_alpha_value = 1.0\n",
        "min_abs_alpha_value = 0.0\n",
        "gamma_correlation = 5.0\n",
        "onehot_temperature = 0.001\n",
        "l2_lambda = 1.0\n",
        "batch_size = 1\n",
        "localization_layers = list(range(1, 18))\n",
        "localization_layer_weights = None\n",
        "log_dir = f'{exp_dir}/cliplsd_stylegan2_ffhq/{latent_space}_{loss_function}/{num_latent_dirs}D/face_bisenet/{part_name}'\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "lelsd = CLIPLSD(device=device,\n",
        "                localization_layers=localization_layers,\n",
        "                semantic_text=\"a photo of a person with glasses\",\n",
        "                loss_function=loss_function,\n",
        "                localization_layer_weights=localization_layer_weights,\n",
        "                n_layers=18,\n",
        "                latent_dim=512,\n",
        "                num_latent_dirs=num_latent_dirs,\n",
        "                learning_rate=lr,\n",
        "                batch_size=batch_size,\n",
        "                gamma_correlation=gamma_correlation,\n",
        "                l2_lambda=l2_lambda,\n",
        "                unit_norm=False,\n",
        "                latent_space=latent_space,\n",
        "                onehot_temperature=onehot_temperature,\n",
        "                min_alpha_value=min_alpha_value,\n",
        "                max_alpha_value=max_alpha_value,\n",
        "                min_abs_alpha_value=min_abs_alpha_value,\n",
        "                log_dir=log_dir,\n",
        "                )\n",
        "lelsd.fit(sample_generator, clip_model, clip_preprocess, num_batches=200 * num_latent_dirs,\n",
        "          num_lr_halvings=3,\n",
        "          pgbar=True, summary=True)\n",
        "lelsd.save()"
      ],
      "metadata": {
        "id": "XienOGuvwJP0",
        "outputId": "00789232-95ce-4aca-b154-0065a3f2b138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "id": "XienOGuvwJP0",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/200 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7753dca3ee94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m lelsd.fit(sample_generator, clip_model, clip_preprocess, num_batches=200 * num_latent_dirs,\n\u001b[1;32m     38\u001b[0m           \u001b[0mnum_lr_halvings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           pgbar=True, summary=True)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mlelsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CLIPLSD/cliplsd.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, gan_sample_generator, clip_model, clip_preprocess, num_batches, num_lr_halvings, batch_size, pgbar, summary, snapshot_interval)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_loss\u001b[0m \u001b[0;31m#+ self.l2_lambda * l2_loss + self.gamma_correlation * correlation_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 512]], which is output 0 of torch::autograd::CopyBackwards, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug Cells"
      ],
      "metadata": {
        "id": "xW-77_Co5ecG"
      },
      "id": "xW-77_Co5ecG"
    },
    {
      "cell_type": "code",
      "source": [
        "batch = sample_generator.generate_batch(seed=1, requires_grad=True, return_image=True)"
      ],
      "metadata": {
        "id": "qr_YCEvxz4SW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qr_YCEvxz4SW"
    },
    {
      "cell_type": "code",
      "source": [
        "batch['raw_image'].size()"
      ],
      "metadata": {
        "id": "4zoiiqPbz_q5",
        "outputId": "caa3350d-3bd5-4a88-b358-8b21639c93ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 3, 1024, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "id": "4zoiiqPbz_q5"
    },
    {
      "cell_type": "code",
      "source": [
        "upsample = torch.nn.Upsample(scale_factor=7)\n",
        "average_pool = torch.nn.AvgPool2d(kernel_size=1024 // 32)\n",
        "image_input = average_pool(upsample(batch['raw_image']))"
      ],
      "metadata": {
        "id": "AaA_mAjJ0PHM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AaA_mAjJ0PHM"
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = clip_model.encode_image(image_input).float().to(device)\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "GCamyvXi0nLc"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GCamyvXi0nLc"
    },
    {
      "cell_type": "code",
      "source": [
        "text_token = clip.tokenize(\"a person with glasses\").to(device)\n",
        "text_features = clip_model.encode_text(text_token).float().to(device)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)"
      ],
      "metadata": {
        "id": "3hm6sv7e03i1"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3hm6sv7e03i1"
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = torch.matmul(text_features, image_features.t())"
      ],
      "metadata": {
        "id": "pyIsm2Zy0yRu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pyIsm2Zy0yRu"
    },
    {
      "cell_type": "code",
      "source": [
        "(1 - similarity).mean()"
      ],
      "metadata": {
        "id": "srptJ1dZ1Oq7",
        "outputId": "8a98808a-00b6-46b8-ea5b-005555fc229a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7726, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "id": "srptJ1dZ1Oq7"
    },
    {
      "cell_type": "code",
      "source": [
        "similarity.view(batch_size, -1)"
      ],
      "metadata": {
        "id": "ipV0QWcd0-dh",
        "outputId": "1fa9fdf6-5da4-4aec-c454-ec8d238dfff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "id": "ipV0QWcd0-dh"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m8CWkep45loI"
      },
      "id": "m8CWkep45loI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "experiment1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}